[
  {
    "objectID": "Project_1.html#collaboration-workflow",
    "href": "Project_1.html#collaboration-workflow",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Collaboration Workflow",
    "text": "Collaboration Workflow\nAs mentioned above, this project was a collaborative effort. The project is housed in a GIT repository to facilitate this. Because of the small size of our team, we had development success working simply off a main branch in GIT. A development branch was created to explore the option of merging, but it proved to be more difficult than necessary for our workflow."
  },
  {
    "objectID": "Project_1.html#libraries",
    "href": "Project_1.html#libraries",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Libraries",
    "text": "Libraries\n\n# Load required libraries \nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(httr)\nlibrary(hms)"
  },
  {
    "objectID": "Project_1.html#primary-user-interface-methods",
    "href": "Project_1.html#primary-user-interface-methods",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Primary User Interface Methods",
    "text": "Primary User Interface Methods\nThe following two functions are designed to take user input and return a properly formatted and easy to use data table or tibble. Before anything further, they will validate each input from the user. They are then passed into functions whose purpose is creating, processing, and handling our API request and response. With user validation checks up front, the helper functions can assume that their inputs are as expected and they can focus on the the more rigorous tasks of transforming user inputs into usable data structures.\nWe will also introduce two further interface methods that allow for additional features for summarizing numeric and categorical data as well as for plotting. These Generic Class Functions will take the tibble of data processed from the census API request as an input to display meaningful summaries and plots depending on the user’s demands.\n\nget_data_tibble_from_census_api\n\nThe first step is to define the inputs we expect from the user. All of our inputs have default settings. As such, this function can be called without arguments and will return census data from the year 2022 from the state of Colorado. In particular, it will return the numeric variables AGEP and PWGTP. As well as the categorical variable SEX.\nNo matter the case, we always pass these inputs to various validation functions. We will discuss each of these in turn below.\nWith valid input from the user, we then set a chain of events to first build the URL from those inputs, then send that request for information from the census. With the data in it’s raw form from the API, we can then work to format it for ease of use.\n\n\n# User interface to take inputs and return fully processed data tibble\nget_data_tibble_from_census_api &lt;- function(year = 2022, \n                                            numeric_vars = c(\"AGEP\", \"PWGTP\"), \n                                            categorical_vars = c(\"SEX\"), \n                                            geography = \"State\", \n                                            subset = \"CO\") {\n  \n  # validate the user inputs\n  validate_year(year)\n  validate_numeric_vars(numeric_vars)\n  validate_categorical_vars(categorical_vars)\n  validate_geography_and_subset(geography, subset)\n  \n  # Send inputs to retrieve data\n  build_query_url(year,\n                  numeric_vars,\n                  categorical_vars,\n                  geography,\n                  subset) |&gt; \n    query_census_with_url()\n}\n\n\n\nquery_census_multiple_years\n\nIn addition to the same arguments as the single year version, this function also takes in a range of years\nA loop calls the single year function with each iteration, adding the results as new elements to a list. Each tibble also has a new column designating the year.\nAfter the loop, bind rows is used to combine all results into one tibble\n\n\n# Function for Querying Multiple Years\nquery_census_multiple_years &lt;- function(years, \n                                 numeric_vars = c(\"AGEP\", \"PWGTP\"), \n                                 categorical_vars = c(\"SEX\"), \n                                 geography = \"State\", \n                                 subset = \"CO\") {\n  \n  # create empty list to store data frames\n  multi_year_list &lt;- list()\n  \n  # call the user interface for each year\n  for (yr in years) {\n    \n    # retrieve single year data tibble\n    census_single_yr &lt;- get_data_tibble_from_census_api(yr,\n                                                        numeric_vars,\n                                                        categorical_vars,\n                                                        geography,\n                                                        subset)\n    \n    # append year to the tibble\n    census_single_yr_tbl &lt;- tibble(Year = yr, census_single_yr)\n    \n    # check how many elements are currently in list\n    elements &lt;- length(multi_year_list)\n    \n    # insert the tbl into the list as the last element\n    multi_year_list[[elements + 1]] &lt;- census_single_yr_tbl\n  }\n  \n  # union of all year-specific results\n  census_multi_year_tbl &lt;- bind_rows(multi_year_list)\n  \n  # return the final tibble\n  return(census_multi_year_tbl)\n}"
  },
  {
    "objectID": "Project_1.html#utilities",
    "href": "Project_1.html#utilities",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Utilities",
    "text": "Utilities\nWe define several helper functions that allow us to fetch particular sets of information whenever required. These facilitate readability, reusability, and our approach to separate concerns.\n\nget_valid_numeric_vars &lt;- function() {\n  c(\"AGEP\", \"PWGTP\", \"GASP\", \"GRPIP\", \"JWAP\", \"JWDP\", \"JWMNP\")\n}\n\nget_valid_categorical_vars &lt;- function() {\n  c(\"SEX\", \"FER\", \"HHL\", \"HISPEED\", \"JWTRNS\", \"SCH\", \"SCHL\")\n}\n\nget_valid_geography_levels &lt;- function() {\n  c(\"All\", \"Region\", \"Division\", \"State\")\n}\n\n\nget_time_refs\n\nThis function does an API call to retrieve the time codes and their associated time ranges in a JSON string and processes them into a tibble, an undertaking that proved arduous and time-consuming.\nThe URL for the API endpoint is actually a .json, so that might be why the data come in differently than the API call for the census data. The values actually came in the form of a list with all 151-286 values as individual elements in that list.\nBind_rows was used to get those elements into a single tibble, so now each named element was its own column. This data table was transposed using pivot_longer to get just 2 columns, one for the time code and the other for the time range\nGetting from the text time ranges, e.g. “8:50 a.m. to 8:54 a.m.” to a nice, clean time (08:52:00) took awhile and a few tries. On the first pass, we assumed both JWAP and JWDP had consistent time ranges, so the first version just took a substring for the first time in the interval, converted it to time, and add 2 minutes to JWAP values and 5 minutes to JWDP values.\nWhile debugging the issue with time code values changing from factor to numeric, we realized that JWDP had varying intervals. The logic was changed to take both the starting and ending times to compute the midpoint of the interval\nSince we had to repeat the same actions on two different strings to get them both in correct time format, these steps were put into a loop. After that, getting the final time and removing the extraneous columns was pretty straightforward.\n\n\nget_time_refs &lt;- function(time_code) {\n  \n  # construct url from the time_code (JWDP or JWAP)\n  time_url &lt;- paste0(\"https://api.census.gov/data/2022/acs/acs1/pums/variables/\",\n                    time_code, \".json\")\n  \n  # retrieve data in list form from API, then bind rows to put in 1 by x tibble,\n  # then transpose the data to get key-value pair in columns\n  times_ref &lt;- \n    fromJSON(time_url)$values |&gt;\n    bind_rows() |&gt;\n    pivot_longer(cols = everything(), \n                 names_to = time_code, \n                 values_to = \"time_range\")\n  \n  # convert 1st column (JWAP/JWDP) to numeric \n  times_ref[[time_code]] &lt;- as.numeric(times_ref[[time_code]])\n  \n  # filter on the row(s) where JWAP/JWDP == 0, change the value for time_range\n  # to missing (it is a string that can't be converted to time, starts with \"N/A\")\n  times_ref$time_range[times_ref[time_code] == 0] &lt;- NA\n  \n  # parse the time_range string to find the start and stop times\n  times_ref &lt;-\n    times_ref |&gt;\n    separate_wider_delim(time_range,\n                         delim = \" to \",\n                         names = c(\"start_time\", \"end_time\"),\n                         cols_remove = FALSE) \n  \n  # convert new start/end columns to time\n  for (col in c(\"start_time\", \"end_time\")) {\n    times_ref[[col]] &lt;-\n      times_ref[[col]] |&gt;\n      toupper() |&gt;                              # change to upper case\n      str_replace_all(\"[.]\", \"\") |&gt;             # remove periods\n      parse_date_time('%I:%M %p', tz = \"EST\")   # convert to date-time\n  }\n\n  # calculate time to the midpoint between the start and end times\n  times_ref &lt;-\n    times_ref |&gt;\n    mutate(midpoint = difftime(end_time, start_time) / 2)\n  \n  # assign new clean time code variable as correct time\n  times_ref[paste0(time_code, \"_clean\")] &lt;-\n    times_ref$start_time + times_ref$midpoint\n\n  # convert format from date-time to time (reference by [[]] not [])\n  times_ref[paste0(time_code, \"_clean\")] &lt;-\n    hms::as_hms(times_ref[[paste0(time_code, \"_clean\")]])\n  \n  # drop extra columns, keeping only 2 (time code, time code clean)\n  times_ref &lt;-\n    times_ref |&gt;\n    select(-time_range, -start_time, -end_time, -midpoint)\n\n  # return final clean ref table\n  return(times_ref)\n}\n\n\n\nget_cat_refs\n\nWhen it was stated in a message on the discussion board that we should convert all categorical values to their meaningful versions, we realized we needed to create reference tables with an API call\nWe were able to utilize our existing get_time_refs function and create a simplified version for the categorical variables.\nThis is a helper function for the function that actually performs the conversion, described below\n\n\nget_cat_refs &lt;- function(cat_code) {\n  \n  # construct url from the time_code (JWDP or JWAP)\n  cat_url &lt;- paste0(\"https://api.census.gov/data/2022/acs/acs1/pums/variables/\",\n                     cat_code, \".json\")\n  \n  # retrieve data in list form from API, then bind rows to put in 1 by x tibble,\n  # then transpose the data to get key-value pair in columns\n  cat_ref &lt;- \n    fromJSON(cat_url)$values |&gt;\n    bind_rows() |&gt;\n    pivot_longer(cols = everything(), \n                 names_to = cat_code, \n                 values_to = \"description\")\n\n  return(cat_ref)\n}\n\n\nget_cat_refs(\"REGION\") |&gt; knitr::kable(align = 'c')\n\n\n\n\nREGION\ndescription\n\n\n\n\n1\nNortheast\n\n\n9\nPuerto Rico\n\n\n3\nSouth\n\n\n2\nMidwest\n\n\n4\nWest\n\n\n\n\n\n\n\nget_subset_code\nIn the construction of the URL, this function will take as arguments the combination of Geography and Subset to return a particular subset’s numeric code used in the URL. If the subset is NULL, we simply return an astrix to be used with any geography the user might provide. Otherwise, we need to retrieve the appropriate subset pertaining to the given geography.\n\nA great opportunity was taken to reuse the get_cat_refs from above here. The regions and divisions had previously been hard-coded within this function, but could now be pulled directly from the API using a single function call.\n\n\nget_subset_code &lt;- function(geography, subset) {\n\n  if (is.null(subset)) {\n    return(\"*\")\n  }\n\n  geography &lt;- tolower(geography)\n\n  # use get_cat_refs\n  region_codes &lt;- get_cat_refs(\"REGION\") |&gt;\n    select(description, REGION)\n  \n  division_codes &lt;- get_cat_refs(\"DIVISION\") |&gt;\n    select(description, DIVISION) |&gt;\n    # Division codes come back with unwanted info in parenthesis\n    mutate(description = str_remove(description, \"\\\\s*\\\\(.*?\\\\)\"))\n\n  # Switch based on geography type\n  switch(geography,\n         \"region\" = region_codes |&gt;\n                    filter(tolower(description) == tolower(subset)) |&gt;\n                    pull(REGION),\n         \"division\" = division_codes |&gt;\n                    filter(tolower(description) == tolower(subset)) |&gt;\n                    pull(DIVISION),\n         \"state\" = get_state_code(subset),  \n         stop(\"Invalid geography type\"))\n}\n\n\nget_subset_code(\"region\", \"south\")\n\n[1] \"3\"\n\nget_subset_code(\"division\", \"mountain\")\n\n[1] \"8\"\n\n\n\n\nget_state_code\nWe have a dedicated helper for states. This function is not only used to retrieve the appropriate numeric code that is to be used in the construction of the URL, it also has a stop() within it that will allow us the validate if the the state input returns a valid code.\n\nAnother fantastic use of get_cat_refs. Initially for get_state_code, the numeric codes and their corresponding states were hardcoded as a named vector and contained within. This was refactored however to greatly reduce the code and increased readability and robustness.\n\n\n# Function to get state code from state name or abbreviation\nget_state_code &lt;- function(state_input) {\n\n  state_input &lt;- tolower(state_input)\n  \n  # Provided state codes\n  state_codes &lt;- get_cat_refs(\"ST\")\n  \n  state_codes_tibble &lt;- state_codes |&gt; \n    separate_wider_delim(description, delim = \"/\", \n                         names = c(\"state\", \"abbreviation\")) |&gt;\n    mutate(state = tolower(state), abbreviation = tolower(abbreviation))\n  \n  # Filter down to match input\n  result &lt;- state_codes_tibble |&gt;\n    filter(state == state_input | abbreviation == state_input) |&gt;\n    pull(ST)\n  \n  # Return the state code or stop if not found\n  if (length(result) == 0) {\n    stop(\"Invalid state name or abbreviation\")\n  }\n  \n  return(result)\n}\n\n\nget_state_code(\"North Carolina\")\n\n[1] \"37\"\n\nget_state_code(\"NC\")\n\n[1] \"37\""
  },
  {
    "objectID": "Project_1.html#input-validation",
    "href": "Project_1.html#input-validation",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Input Validation",
    "text": "Input Validation\nWe want to be able to fail early and give pertinent information back to the user if inputs are wrong or if an error occurs. These validation checks serve that purpose. If an input is wrong, we can often display to the user which input was incorrect and what the valid options are.\nEnsure that the year is between 2010 and 2022\n\n# Year must be between 2010 and 2022.\nvalidate_year &lt;- function(year){\n  \n  if (!(year %in% 2010:2022)) \n    stop(\"Year must be between 2010 and 2022.\")\n}\n\n\nvalidate_year(2000) |&gt; catch_error()\n\nCaught error: Year must be between 2010 and 2022.\n\n\nHere we handle the numeric variables. PWGTP and at least one other valid numeric variable must be selected. We have our lists of valid inputs in the utility functions section for reference. We will check against these to ensure each variable in the input is valid.\n\n# PWGTP and at least one other valid numeric variable must be selected\nvalidate_numeric_vars &lt;- function(numeric_vars) {\n  \n  # Not worried about case\n  valid_numeric_vars &lt;- toupper(get_valid_numeric_vars())\n  numeric_vars &lt;- toupper(numeric_vars)\n\n  numeric_vars &lt;- intersect(numeric_vars, valid_numeric_vars)\n  \n  if (length(numeric_vars) &lt; 2 || !\"PWGTP\" %in% numeric_vars) {\n    stop(\"PWGTP and at least one other numeric variable must be selected from: \", \n         paste(valid_numeric_vars, collapse = \", \"))\n  }\n}\n\n\nvalidate_numeric_vars(c(\"Invalid\")) |&gt; catch_error()\n\nCaught error: PWGTP and at least one other numeric variable must be selected from: AGEP, PWGTP, GASP, GRPIP, JWAP, JWDP, JWMNP\n\n\nIn a similar fashion, we handle categorical variable inputs. We expect at least one categorical variable from the user. It should be noted, that we are not so much interested in the case of the variables. The URL does have a standard for case, but it does not cause an error in testing. Thus, we allow it for a better user experience.\n\n# At least one valid categorical variable must be selected\nvalidate_categorical_vars &lt;- function(categorical_vars) {\n  \n  # Not worried about case\n  valid_categorical_vars &lt;- toupper(get_valid_categorical_vars())\n  categorical_vars &lt;- toupper(categorical_vars)\n  \n  categorical_vars &lt;- intersect(categorical_vars, valid_categorical_vars)\n  \n  if (length(categorical_vars) &lt; 1) {\n    stop(\"At least one valid categorical variable must be selected from: \", \n         paste(valid_categorical_vars, collapse = \", \"))\n  }\n}\n\n\nvalidate_categorical_vars(c(\"Invalid\")) |&gt; catch_error()\n\nCaught error: At least one valid categorical variable must be selected from: SEX, FER, HHL, HISPEED, JWTRNS, SCH, SCHL\n\n\n\nvalidate_geography_and_subset\nGeography and subset are a bit more complicated, but the general idea still applies. We want to check the input against valid lists. Firstly we need to check geography. Given a valid geography, there are valid subsets that can be accepted. The subset can be NULL for any choice of geography.\nSelecting the State geography in particular is worth mentioning. We developed a helper for states because we wanted to allow the user to provide a state name or abbreviation instead of a state code. In this way it is handled slightly different from the other geographies.\n\na quick note that get_cat_refs could potentially be implemented here as well to refactor the hardoded region and divisions\n\n\n# Geography & Subset Together\nvalidate_geography_and_subset &lt;- function(geography, subset) {\n  \n  # Handle case\n  valid_geography_levels &lt;- tolower(get_valid_geography_levels())\n  geography &lt;- tolower(geography)\n  \n  # Validate the geography\n  if (!(geography %in% valid_geography_levels)) {\n    stop(\"Invalid geography level. Must be one of: \", \n         paste(valid_geography_levels, collapse = \", \"))\n  }\n  \n  # If geography is \"all\", subsetting is not allowed\n  if (geography == \"all\" && !is.null(subset)) {\n    stop(\"Subsetting is not allowed when geography is 'All'.\")\n  }\n  \n  valid_region_division_options &lt;- list(\n    region = tolower(c(\"Northeast\", \"Midwest\", \"South\", \"West\")),\n    division = tolower(c(\"New England\", \"Middle Atlantic\", \n                         \"East North Central\", \"West North Central\", \n                         \"South Atlantic\", \"East South Central\", \n                         \"West South Central\", \"Mountain\", \"Pacific\"))\n  )\n  \n  # Check for region and division\n  if (geography %in% c(\"region\", \"division\")) {\n    if (!(tolower(subset) %in% valid_region_division_options[[geography]])) {\n      stop(\"Invalid \", geography, \". Must be one of: \", \n           paste(valid_region_division_options[[geography]], collapse = \", \"))\n    }\n  }\n  \n  # Handle State geography\n  if (geography == \"state\") {\n    # get_state_code will err out if invalid\n    state_code &lt;-  get_state_code(subset)\n  }\n}\n\n\nvalidate_geography_and_subset(\"region\", \"Invalid\") |&gt; catch_error()\n\nCaught error: Invalid region. Must be one of: northeast, midwest, south, west\n\nvalidate_geography_and_subset(\"Invalid\", NULL) |&gt; catch_error()\n\nCaught error: Invalid geography level. Must be one of: all, region, division, state\n\n\n\n\nvalidate_url_response\nIt was discovered that some URL’s do not return data. This caused errors down the line when the functions used to process the data were expecting their own inputs. To handle this issue more gracefully, we built a check on the response from the API.\n\nWe first want to check that the status was a success. If we are dealing with more than just empty data, we can actually display the error message to further diagnose the issue.\nIf the request was a success, we also check that the API returned anything at all for us to process. If nothing came back, there is no need to proceed.\nIt was also discovered that the API might reject or return an error message even if the request was “successful”. In such a case, we shold notify the user of this as well.\n\n\n# Check we got something from the API using GET(URL)\nvalidate_url_response &lt;- function(response) {\n  \n  is_success &lt;- http_status(response)$category == \"Success\"\n  \n  response_content &lt;- content(response, as = \"text\")\n  has_content &lt;- !is.null(response_content) && nchar(response_content) &gt; 0\n  \n  error_in_content &lt;- str_detect(tolower(response_content), \n                                 \"error|not found|invalid|rejected\")\n\n  if (!is_success || !has_content || error_in_content) {\n    stop(\"API request failed: \", \n           if (!is_success) {\n             http_status(census_raw)$message \n           } else if (!has_content) {\n             \"Empty response from API.\"\n           } else {\n             \"Error returned from API\"\n           }\n         )\n  }\n  \n  # print to console to show progression in request\n  print(\"API request successful\")\n}\n\n\nvalidate_url_response(httr::GET(\"https://api.census.gov/invalid\")) |&gt;\n  catch_error()\n\nCaught error: API request failed: Error returned from API"
  },
  {
    "objectID": "Project_1.html#building-the-url",
    "href": "Project_1.html#building-the-url",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Building the URL",
    "text": "Building the URL\nHere we reach our first undertaking after validating inputs. We need to construct a URL from the user input.\n\nWe ran into some issues with fetching data from various years. It was discovered that by altering our base url dependent upon the year provided, we were able to query recent years and older records in different manners alleviating the issue. This is seen in the dataset_type in the code below.\nWe then piece together all the desired variables to append to the base URL.\nGeography and subsets are at the end. Here we very much leverage the helper/utility functions that we defined above.\n\n\n# Build a valid URL for the Census API\nbuild_query_url &lt;- function(year = 2022, \n                            numeric_vars = c(\"AGEP\", \"PWGTP\"), \n                            categorical_vars = c(\"SEX\"), \n                            geography = \"State\", \n                            subset = \"CO\") {\n\n  dataset_type &lt;- ifelse(year == 2021 || year == 2022, \"acs1\", \"acs5\")\n  \n  base_url &lt;- paste0(\"https://api.census.gov/data/\", \n                     year, \"/acs/\", dataset_type, \"/pums?\")\n  \n  \n  # Handle numeric and categorical inputs\n  query_vars &lt;- c(numeric_vars, categorical_vars)\n  query_string &lt;- paste0(\"get=\", paste(query_vars, collapse = \",\"))\n  \n  # Handle geography levels (\"All\" will require no 'for' clause)\n  geography_query &lt;- \"\"\n  \n  if (geography != \"All\") {\n\n    # Subsets need to be numeric codes. If null will return *\n    subset &lt;- get_subset_code(geography, subset)\n    \n    geography_query &lt;- paste0(\"for=\", gsub(\" \", \"%20\", geography), \":\", subset)\n  }\n  \n  # Concatenate base_url, query_string, and geography_query\n  final_url &lt;- paste0(base_url, query_string)\n  \n  if (geography_query != \"\") {\n    final_url &lt;- paste0(final_url, \"&\", geography_query)\n  }\n  \n  # print to console to show progression in request\n  cat(\"URL: \", final_url)\n  \n  return(final_url)\n}\n\n\ntest_url &lt;- build_query_url()\n\nURL:  https://api.census.gov/data/2022/acs/acs1/pums?get=AGEP,PWGTP,SEX&for=State:08"
  },
  {
    "objectID": "Project_1.html#processing-the-response",
    "href": "Project_1.html#processing-the-response",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Processing the Response",
    "text": "Processing the Response\n\nquery_census_with_url\nWe began with the instructions from the lecture to query the API and transform the JSON string into a data frame with the raw data\n\nA series of helper functions were developed to keep better track of what each step within this function was doing, which helped to focus on small tasks rather than getting overwhelmed by everything that had to be done to get the final clean data\nThe data frame that came out of the JSON-to-data-frame helper had all the data in columns, but everything was character, so a final helper function takes care of cleaning the data\n\n\nquery_census_with_url &lt;- function(url) {\n  \n  # retrieve data in list form from API\n  census_raw &lt;- httr::GET(url)\n  \n  # check that data was returned\n  validate_url_response(census_raw)\n  \n  # call helper function to turn API raw data into a raw tibble\n  census_raw_tbl &lt;- json_to_raw_tbl_helper(census_raw)\n\n  # call helper function to clean tibble\n  census_clean_tbl &lt;- process_census_data(census_raw_tbl)\n  \n  # return final clean tibble\n  return(census_clean_tbl)\n}\n\n\n\njson_to_raw_tbl_helper\nThis is a helper function for query_census_with_url, and puts json stuff into a raw tibble. All variables are character\n\nThe data frame that was created from JSON had the column names in the first row, so a step was included to grab the first row as the column names, and take the rest of the rows as the observations\n\n\njson_to_raw_tbl_helper &lt;- function(census_raw) {\n  \n  # convert JSON string raw data to data frame (first row are column names)\n  parsed_census &lt;- as.data.frame(fromJSON(rawToChar(census_raw$content)))\n  \n  # convert to a tibble, use 1st row from raw df as column names\n  census_tbl &lt;- as_tibble(parsed_census[-1,])\n  colnames(census_tbl) &lt;- toupper(parsed_census[1,])\n  \n  # return final tibble\n  return(census_tbl)\n}\n\n\n\nprocess_census_data\nThis function is responsible for taking the character data in the tibble, and converting them to the appropriate data types.\n\nIt was straight-forward to coerce categorical fields into factors, and the regular numeric fields into numeric.\nIt took a lot of debugging and research to get the time fields right.\n\nAt first, they were converted to factors since JWAP/JWDP were both also initially considered both numerical and categorical variables.\nHowever, we noticed a lot of values were close to midnight. Further investigation found that a lot of values that were actually 0 were getting changed to 1 somewhere along the way.\nWe looked to see if there was something wrong with the join to the time reference table (which provided a time for each time code), and it looked right.\nHowever, we discovered that when they were coerced directly from factors to numeric, all of the values changed, since what we were changing were actually the cardinal levels and not the level description. So “0” was changing to 1 because it was the lowest (first) level. This of course meant that all of the values were now wrong.\nSince Dr. Post confirmed that JWAP/JWDP didn’t need to be categorical values, we removed the step that was converting them to factors, and the values now converted (from character) as expected.\n\n\n\nprocess_census_data &lt;- function(census_data_tbl) {\n\n  # if state is a column, rename as ST\n  if (\"STATE\" %in% names(census_data_tbl)) {\n    names(census_data_tbl)[names(census_data_tbl) == \"STATE\"] &lt;- \"ST\"\n  }\n  \n  # retrieve valid categorical variables\n  cat_vars &lt;- \n    get_valid_categorical_vars() |&gt;      # get all valid categorical variables\n    c(\"DIVISION\", \"REGION\", \"ST\") |&gt;     # append regional categories\n    intersect(names(census_data_tbl))    # keep only values in the data set\n  \n  # convert categorical variables to actual descriptive values, and as factors\n  for (var in cat_vars){\n    census_data_tbl[var] &lt;- convert_cat_code_to_description(census_data_tbl[var])\n  } \n  \n  # retrieve valid numeric vars, keeping only the ones that exist in \n  # the input raw data (note JWAP and JWDP will still need to be changed to times)\n  num_vars &lt;- \n    get_valid_numeric_vars() |&gt;\n    intersect(names(census_data_tbl))\n  \n  # turn vars into numeric values in the tibble \n  for (var in num_vars){\n    census_data_tbl[[var]] &lt;- as.numeric(census_data_tbl[[var]])\n  } \n  \n  # collect the time variables to convert\n  time_vars &lt;- \n    c(\"JWAP\", \"JWDP\") |&gt;\n    intersect(names(census_data_tbl))\n  \n  # call helper function to convert time codes to numeric time (won't run if \n  # time_vars is empty)\n  for (time_code in time_vars) {\n    census_data_tbl &lt;- convert_num_code_to_time(census_data_tbl, time_code)\n  }\n  \n  # Assign class for custom methods\n  class(census_data_tbl) &lt;- c(\"census\", class(census_data_tbl))\n  \n  # return clean tibble\n  return(census_data_tbl)\n}\n\n\n\nconvert_num_code_to_time\nThis is another helper function to take the numerical time codes and change them to a meaningful time\n\nIt starts with another helper function to get a reference tibble that provides the key-value pairs for each time code and time\nIt takes the census data and left joins to the time reference table, and replaces the JWAP raw value with the actual time. Using a variable as one of the keys in the “by” argument proved elusive, so we kept it as a natural join. It’s not ideal, but it does work. It meant the names of the primary and foreign keys in each table had to match (i.e. JWAP or JWDP), and that no other columns between the two tables could share the same name.\nThe final step was to replace the values in JWAP/JWDP with the correct time from the 2nd table, and drop the now duplicate field that had been pulled in (named JWAP_clean or JWDP_clean)\n\n\nconvert_num_code_to_time &lt;- function(census_data_tbl, time_code) {\n  \n  # get time references from API`\n  times_reference &lt;- get_time_refs(time_code)\n  \n  # join new JWAP/JWDP to table with proper times\n  census_data_tbl &lt;- \n    census_data_tbl |&gt;\n    left_join(times_reference) # natural join on time code\n    \n  # assign the cleaned time values to the JWAP/JWDP column\n  census_data_tbl[time_code] &lt;- census_data_tbl[paste0(time_code, \"_clean\")]\n  \n  # Drop the extra column from the time reference table\n  census_data_tbl &lt;- census_data_tbl |&gt;\n    select(-one_of(paste0(time_code, \"_clean\")))\n  \n  return(census_data_tbl)\n}\n\n\n\nconvert_cat_code_to_description\nThe job of this function is to convert the raw values for categorical variables in the data, and change them to the meaningful descriptive values using a reference table\n\nSince it appeared sometimes raw data seemed to omit leading zeroes, we decided to remove all leading zeroes to ensure the joins matched values appropriately. This was accomplished by coercing the character values to numeric, and then back to character.\n\n\n# take categorical raw value and convert to descriptive value, and make a factor\nconvert_cat_code_to_description &lt;- function(data_column) {\n\n  # get the variable name that has to be looked up\n  var &lt;- colnames(data_column)\n    \n  # get time references from API`\n  cat_reference &lt;- get_cat_refs(var)\n  \n  # remove leading zeroes (census raw data sometimes have them, sometimes don't)\n  cat_reference[[var]] &lt;- as.character(as.numeric(cat_reference[[var]]))\n  \n  # remove leading zeroes from the data column too\n  data_column[[var]] &lt;- as.character(as.numeric(data_column[[var]]))\n  \n  # join new lookup table to the data column with proper values\n  data_column &lt;- \n    data_column |&gt;\n    left_join(cat_reference) # natural join on coded value\n  \n  # return new data column with descriptive values, as factor\n  return(as.factor(data_column[[2]]))\n}"
  },
  {
    "objectID": "Project_1.html#summary",
    "href": "Project_1.html#summary",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Summary",
    "text": "Summary\nBy default, it will summarize all numeric variables (other than PWGTP) and all categorical variables found in the tibble provided. The user is able to specify the variables they’d like in particular as additional arguments if all are not required.\n\nNumeric variables will have their weighted sample means and standard deviations calculated.\nCategorical variables will have their level counts displayed.\nTime variables are considered numeric here, but they need to be handled with some care. We convert them from their HMS form into seconds from midnight to perform calculations on them. We then convert back to HMS so that they are in a more friendly format.\nThere are loops in this function that traverse across the columns of data and summarize them accordingly. Within each iteration of the loop, we utilize vectorized functions to process and make calculations across all the observations in the vector of values for that column/variable. Since there are only a small number of variables possible, these loops were left intact for readability. This however, could be a future refactoring opportunity as we might wish to avoid loops in R altogether.\n\n\n# Summary Function for Census Class\nsummary.census &lt;- function(data, \n                           numeric_vars = NULL, \n                           categorical_vars = NULL) {\n  \n  # Determine the variables that are actually in the dataset\n  valid_numeric_vars &lt;- get_valid_numeric_vars()\n  valid_categorical_vars &lt;- get_valid_categorical_vars()\n  \n  data_names &lt;- toupper(names(data))\n  \n  numeric_vars_in_data &lt;- intersect(data_names, valid_numeric_vars)\n  categorical_vars_in_data &lt;- intersect(data_names, valid_categorical_vars)\n  \n  # Default: Summarize all numeric variables except PWGTP in dataset\n  if (is.null(numeric_vars)) {\n    numeric_vars &lt;- numeric_vars_in_data[numeric_vars_in_data != \"PWGTP\"]\n  } else {\n    # otherwise filter only for those provided\n    numeric_vars &lt;- intersect(toupper(numeric_vars), numeric_vars_in_data)\n  }\n  \n  # Default: Summarize all categorical variables in dataset\n  if (is.null(categorical_vars)) {\n    categorical_vars &lt;- categorical_vars_in_data\n  } else {\n    # otherwise filter only for those provided\n    categorical_vars &lt;- intersect(toupper(categorical_vars),\n                                  categorical_vars_in_data)\n  }\n  \n  weight &lt;- data$PWGTP\n  summary_list &lt;- list()\n\n  # Summarize numeric variables\n  for (var in numeric_vars) {\n    \n    # Check if the variable is a time variable\n    is_time_var &lt;- var %in% c(\"JWAP\", \"JWDP\")\n    \n    if (is_time_var) {\n      # Convert time to seconds\n      numeric_vector &lt;- as.numeric(data[[var]])\n    } else if (is.numeric(data[[var]])) {\n      numeric_vector &lt;- data[[var]]\n    } else {\n      stop(\"Unexpected non-numeric variable: \", var)\n    }\n    \n    # Calculate weighted mean and standard deviation\n    weighted_sample_mean &lt;- sum(numeric_vector * weight, na.rm = TRUE) / \n                              sum(weight, na.rm = TRUE)\n    sample_sd &lt;- sqrt(sum((numeric_vector^2) * weight, na.rm = TRUE) / \n                        sum(weight, na.rm = TRUE) - weighted_sample_mean^2)\n    \n    if (is_time_var) {\n      # Convert the results back to hms\n      weighted_sample_mean &lt;- as_hms(weighted_sample_mean) \n      sample_sd &lt;- as_hms(sample_sd)\n    }\n    \n    # Store the results\n    summary_list[[var]] &lt;- list(\n      mean = weighted_sample_mean,\n      sd = sample_sd\n    )\n  }\n  \n  # Summarize categorical variables\n  for (var in categorical_vars) {\n    \n    counts &lt;-  data |&gt; count(.data[[var]])\n    \n    # Store the results\n    summary_list[[var]] &lt;- list(\n      counts = counts\n    )\n  }\n\n  return(summary_list)\n}"
  },
  {
    "objectID": "Project_1.html#plotting",
    "href": "Project_1.html#plotting",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Plotting",
    "text": "Plotting\nA Generic plot() function was also created for a census class tibble. This requires the user to specify one categorical variable and one numeric variable for plotting purposes.\n\nHere we check the inputs are in the data and filter out and missing entries before plotting.\nThe biggest point of note is the use of sampling. Some of the data sets that come through the API can contain millions of records. Particularly if the region is set to All. Due to this circumstance, the plotting was found to take a considerable amount of time. To address this, we simply evaluate the number of rows in the data, and if necessary, take a random sample for plotting purposes.\n\n\n# Plotting Function for Census Class \nplot.census &lt;- function(data, \n                        numeric_var, \n                        categorical_var,\n                        sample_size = 100000) {\n  \n  # Check User inputs\n  for (var in c(numeric_var, categorical_var, \"PWGTP\")) {\n    if (!var %in% names(data)) {\n      stop(paste(\"The variable\", var, \"is not present in the dataset. \",\n                 \"Select from: \", paste(names(data), collapse = \", \")))\n    }\n  }\n  \n  # Remove NA records\n  data &lt;- data |&gt;\n    filter(!is.na(.data[[numeric_var]]) & \n             !is.na(.data[[categorical_var]]))\n  \n  # If the dataset is large, take a random sample\n  if (nrow(data) &gt; sample_size) {\n    message(nrow(data), \" records found in dataset. Sampled \", \n            sample_size, \" rows for plotting.\")\n    set.seed(123)\n    data &lt;- data |&gt; sample_n(sample_size)\n  }\n  \n  # Plot with ggplot2\n  ggplot(data, \n         aes(x = get(categorical_var), \n             y = get(numeric_var), \n             weight = PWGTP)) +\n    geom_boxplot(fill = \"lightsteelblue\", color = \"black\") +\n    labs(\n      title = paste(\"Boxplot of\", numeric_var, \"by\", categorical_var),\n      x = categorical_var, \n      y = numeric_var \n    ) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n}"
  },
  {
    "objectID": "Project_1.html#single-year",
    "href": "Project_1.html#single-year",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Single Year",
    "text": "Single Year\nFirst, we demonstrate the output of the main function using default values.\n\ntest_with_defaults &lt;- get_data_tibble_from_census_api()\n\nURL:  https://api.census.gov/data/2022/acs/acs1/pums?get=AGEP,PWGTP,SEX&for=State:08[1] \"API request successful\"\n\n\nJoining with `by = join_by(SEX)`\nJoining with `by = join_by(ST)`\n\ntest_with_defaults |&gt;\n  head() |&gt;\n  knitr::kable(align = 'c')\n\n\n\n\nAGEP\nPWGTP\nSEX\nST\n\n\n\n\n18\n70\nFemale\nColorado/CO\n\n\n35\n41\nMale\nColorado/CO\n\n\n40\n35\nMale\nColorado/CO\n\n\n20\n30\nFemale\nColorado/CO\n\n\n14\n4\nMale\nColorado/CO\n\n\n28\n75\nFemale\nColorado/CO\n\n\n\n\n\nWithout supplying specific categorical and numeric values, note how the summary function returns results for all numeric variables except for the weight, along with all categorical variables.\n\nsummary.census(test_with_defaults)\n\n$AGEP\n$AGEP$mean\n[1] 38.80219\n\n$AGEP$sd\n[1] 22.42793\n\n\n$SEX\n$SEX$counts\n# A tibble: 2 × 2\n  SEX        n\n  &lt;fct&gt;  &lt;int&gt;\n1 Female 29901\n2 Male   29940\n\n\nRecall that our plot requires one numeric and one categorical variable, so since our data in this example were generated from defaults, we used Age for the numeric variable, and Sex for the categorical variable.\n\nplot.census(test_with_defaults, \"AGEP\", \"SEX\")\n\n\n\n\n\n\n\n\n\nFor our investigative example, we have chosen to examine how arrival time to work (JWAP) is related to household language (HHL) in the state of North Carolina for the year 2015.\n\ntest_with_vars &lt;- get_data_tibble_from_census_api(2015,\n                                  c(\"PWGTP\", \"JWAP\"),\n                                  c(\"HHL\"),\n                                  \"state\",\n                                  \"nc\")\n\nURL:  https://api.census.gov/data/2015/acs/acs5/pums?get=PWGTP,JWAP,HHL&for=state:37[1] \"API request successful\"\n\n\nJoining with `by = join_by(HHL)`\nJoining with `by = join_by(ST)`\nJoining with `by = join_by(JWAP)`\n\ntest_with_vars |&gt;\n  head() |&gt;\n  knitr::kable(align = 'c')\n\n\n\n\nPWGTP\nJWAP\nHHL\nST\n\n\n\n\n3\nNA\nEnglish Only\nNorth Carolina/NC\n\n\n3\nNA\nEnglish Only\nNorth Carolina/NC\n\n\n5\nNA\nEnglish Only\nNorth Carolina/NC\n\n\n6\nNA\nEnglish Only\nNorth Carolina/NC\n\n\n13\n04:07:00\nSpanish\nNorth Carolina/NC\n\n\n12\nNA\nSpanish\nNorth Carolina/NC\n\n\n\n\n\nThe summary for our example tibble appears below.\n\nsummary.census(test_with_vars)\n\n$JWAP\n$JWAP$mean\n03:45:33.0226\n\n$JWAP$sd\n04:58:32.467112\n\n\n$HHL\n$HHL$counts\n# A tibble: 6 × 2\n  HHL                                     n\n  &lt;fct&gt;                               &lt;int&gt;\n1 Asian and Pacific Island languages   8659\n2 English Only                       405007\n3 N/A (GQ/vacant)                     24922\n4 Other Indo-European languages       11492\n5 Other Language                       2921\n6 Spanish                             33903\n\n\nThe boxplots show that the center of the distributions are all very similar, regardless of household language. There is more variation in the interquartile ranges for the different languages.\n\nplot.census(test_with_vars, \"JWAP\", \"HHL\")\n\n198175 records found in dataset. Sampled 1e+05 rows for plotting."
  },
  {
    "objectID": "Project_1.html#multi-year",
    "href": "Project_1.html#multi-year",
    "title": "Query Functions for Public Use Microdata Sample Census API",
    "section": "Multi Year",
    "text": "Multi Year\nTo showcase our multi-year function, we will view the gross rent as a percentage of household income over the last 12 months (GRPIP) in Wyoming, from 2017 and 2018, by whether they have given birth within the last 12 months (FER). We display an aggregate of the results, so that the stratification by year can be seen.\n\ntest_multi_year &lt;- query_census_multiple_years(2017:2018, \n                                               c(\"PWGTP\", \"GRPIP\"), \n                                               \"FER\", \n                                               geography = \"State\", \n                                               subset = \"Wyoming\")\n\nURL:  https://api.census.gov/data/2017/acs/acs5/pums?get=PWGTP,GRPIP,FER&for=State:56[1] \"API request successful\"\n\n\nJoining with `by = join_by(FER)`\nJoining with `by = join_by(ST)`\n\n\nURL:  https://api.census.gov/data/2018/acs/acs5/pums?get=PWGTP,GRPIP,FER&for=State:56[1] \"API request successful\"\n\n\nJoining with `by = join_by(FER)`\nJoining with `by = join_by(ST)`\n\ntest_multi_year |&gt;\n  group_by(Year, ST, FER) |&gt;\n  summarize(mean_GRPIP = sum(PWGTP*GRPIP)/sum(PWGTP)) |&gt;\n  knitr::kable(align = 'c')\n\n`summarise()` has grouped output by 'Year', 'ST'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nYear\nST\nFER\nmean_GRPIP\n\n\n\n\n2017\nWyoming/WY\nN/A (less than 15 years/greater than 50 years/ male)\n7.610905\n\n\n2017\nWyoming/WY\nNo\n10.687256\n\n\n2017\nWyoming/WY\nYes\n13.154172\n\n\n2018\nWyoming/WY\nN/A (less than 15 years/greater than 50 years/ male)\n7.515208\n\n\n2018\nWyoming/WY\nNo\n10.709837\n\n\n2018\nWyoming/WY\nYes\n11.894927"
  }
]